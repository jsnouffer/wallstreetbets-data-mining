{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe7bc59-012d-47b2-bd46-08d59fe34e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "DOWNLOAD_NLTK = False\n",
    "\n",
    "if DOWNLOAD_NLTK:\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ffe30c-2e6f-4cd4-a7a7-fabbeda7e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e252a2a2-c498-4d03-9f33-60de2ab965f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/jason/mbti_model/mbti_1.csv\")\n",
    "df = df.head(100)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48493b0-8ab7-4e32-9618-28f0644b55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Remove the stop words for speed\n",
    "useless_words = stopwords.words(\"english\")\n",
    "\n",
    "# Remove these from the posts\n",
    "unique_type_list = [\n",
    "    \"INFJ\",\n",
    "    \"ENTP\",\n",
    "    \"INTP\",\n",
    "    \"INTJ\",\n",
    "    \"ENTJ\",\n",
    "    \"ENFJ\",\n",
    "    \"INFP\",\n",
    "    \"ENFP\",\n",
    "    \"ISFP\",\n",
    "    \"ISTP\",\n",
    "    \"ISFJ\",\n",
    "    \"ISTJ\",\n",
    "    \"ESTP\",\n",
    "    \"ESFP\",\n",
    "    \"ESTJ\",\n",
    "    \"ESFJ\",\n",
    "]\n",
    "unique_type_list = [x.lower() for x in unique_type_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "180d37a8-5049-4d89-aa40-7a5b462de75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the MBTI personality into 4 letters and binarizing it\n",
    "\n",
    "b_Pers = {\"I\": 0, \"E\": 1, \"N\": 0, \"S\": 1, \"F\": 0, \"T\": 1, \"J\": 0, \"P\": 1}\n",
    "b_Pers_list = [{0: \"I\", 1: \"E\"}, {0: \"N\", 1: \"S\"}, {0: \"F\", 1: \"T\"}, {0: \"J\", 1: \"P\"}]\n",
    "\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "\n",
    "# To show result output for personality prediction\n",
    "def translate_back(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in df.type])\n",
    "# print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06140e85-7e84-42e7-b702-0082bb38c77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example :\n",
      "\n",
      "MBTI before preprocessing:\n",
      "\n",
      " INFJ\n",
      "\n",
      "MBTI after preprocessing:\n",
      "\n",
      " [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    len_data = len(data)\n",
    "    i = 0\n",
    "\n",
    "    for row in data.iterrows():\n",
    "        # check code working\n",
    "        # i+=1\n",
    "        # if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "        #     print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "        # Remove and clean comments\n",
    "        posts = row[1].posts\n",
    "\n",
    "        # Remove url links\n",
    "        temp = re.sub(\n",
    "            \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "            \" \",\n",
    "            posts,\n",
    "        )\n",
    "\n",
    "        # Remove Non-words - keep only words\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "\n",
    "        # Remove spaces > 1\n",
    "        temp = re.sub(\" +\", \" \", temp).lower()\n",
    "\n",
    "        # Remove multiple letter repeating words\n",
    "        temp = re.sub(r\"([a-z])\\1{2,}[\\s|\\w]*\", \"\", temp)\n",
    "\n",
    "        # Remove stop words\n",
    "        if remove_stop_words:\n",
    "            temp = \" \".join(\n",
    "                [\n",
    "                    lemmatiser.lemmatize(w)\n",
    "                    for w in temp.split(\" \")\n",
    "                    if w not in useless_words\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(\" \")])\n",
    "\n",
    "        # Remove MBTI personality words from posts\n",
    "        if remove_mbti_profiles:\n",
    "            for t in unique_type_list:\n",
    "                temp = temp.replace(t, \"\")\n",
    "\n",
    "        # transform mbti to binary vector\n",
    "        type_labelized = translate_personality(\n",
    "            row[1].type\n",
    "        )  # or use lab_encoder.transform([row[1].type])[0]\n",
    "        list_personality.append(type_labelized)\n",
    "        # the cleaned data temp is passed here\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    # returns the result\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality\n",
    "\n",
    "\n",
    "list_posts, list_personality = pre_process_text(\n",
    "    df, remove_stop_words=True, remove_mbti_profiles=True\n",
    ")\n",
    "\n",
    "print(\"Example :\")\n",
    "# print(\"\\nPost before preprocessing:\\n\\n\", data.posts[0])\n",
    "# print(\"\\nPost after preprocessing:\\n\\n\", list_posts[0])\n",
    "print(\"\\nMBTI before preprocessing:\\n\\n\", df.type[0])\n",
    "print(\"\\nMBTI after preprocessing:\\n\\n\", list_personality[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab81e0ce-322a-4915-8d83-044e37722751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of posts = 100  and No. of Personalities = 4 \n"
     ]
    }
   ],
   "source": [
    "nRow, nCol = list_personality.shape\n",
    "print(f'No. of posts = {nRow}  and No. of Personalities = {nCol} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac0540c-a14a-43d6-adc7-9d26d58c7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer :\n",
      "10 feature names can be seen below\n",
      "[(0, 'ability'), (1, 'able'), (2, 'absolutely'), (3, 'accurate'), (4, 'achieve'), (5, 'act'), (6, 'action'), (7, 'actually'), (8, 'add'), (9, 'admit')]\n",
      "\n",
      "Using Tf-idf :\n",
      "Now the dataset size is as below\n",
      "(100, 636)\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing the database posts to a matrix of token counts for the model\n",
    "cntizer = CountVectorizer(analyzer=\"word\", max_features=1000, max_df=0.7, min_df=0.1)\n",
    "# the feature should be made of word n-gram\n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"Using CountVectorizer :\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "\n",
    "# The enumerate object yields pairs containing a count and a value (useful for obtaining an indexed list)\n",
    "feature_names = list(enumerate(cntizer.get_feature_names()))\n",
    "print(\"10 feature names can be seen below\")\n",
    "print(feature_names[0:10])\n",
    "\n",
    "# For the Standardization or Feature Scaling Stage :-\n",
    "# Transform the count matrix to a normalized tf or tf-idf representation\n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "print(\"\\nUsing Tf-idf :\")\n",
    "\n",
    "print(\"Now the dataset size is as below\")\n",
    "X_tfidf = tfizer.fit_transform(X_cnt).toarray()\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0563794e-13fa-45ec-ae0a-21921b71ce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E)\n",
      "NS: Intuition (N) / Sensing (S)\n",
      "FT: Feeling (F) / Thinking (T)\n",
      "JP: Judging (J) / Perceiving (P)\n"
     ]
    }
   ],
   "source": [
    "personality_type = [\n",
    "    \"IE: Introversion (I) / Extroversion (E)\",\n",
    "    \"NS: Intuition (N) / Sensing (S)\",\n",
    "    \"FT: Feeling (F) / Thinking (T)\",\n",
    "    \"JP: Judging (J) / Perceiving (P)\",\n",
    "]\n",
    "\n",
    "for l in range(len(personality_type)):\n",
    "    print(personality_type[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "824fe523-7cf2-48a0-b031-575b04992a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MBTI personality type : INFJ\n",
      "Y : Binarized MBTI 1st row: [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"For MBTI personality type : %s\" % translate_back(list_personality[0,:]))\n",
    "print(\"Y : Binarized MBTI 1st row: %s\" % list_personality[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d24d035-f9e5-42bf-8a2e-169af383393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posts in tf-idf representation\n",
    "X = X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfda6eee-f818-4ba8-bbdc-dfe50ab3a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 20:22:05.152007: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-23 20:22:05.152062: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1788 > 512). Running this sequence through the model will result in indexing errors\n",
      "/tmp/ipykernel_22859/1208527064.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np.array(x_token), Y, test_size=0.33, random_state=7\n",
      "/home/jason/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please reshape the input data X into 2-dimensional matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22859/1208527064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# fit model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# make predictions for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;31m# Simply raise an error here since there might be many\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0;31m# different ways of reshaping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please reshape the input data X into 2-dimensional matrix.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "\u001b[0;31mValueError\u001b[0m: Please reshape the input data X into 2-dimensional matrix."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\") # \"bert-large-uncased\"\n",
    "\n",
    "for l in range(len(personality_type)):\n",
    "\n",
    "    Y = list_personality[:, l]\n",
    "\n",
    "    x_token = [\n",
    "        tokenizer.encode(str(i)) # max_length=maxlen, pad_to_max_length=False, truncation=True)\n",
    "        for i in X\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        np.array(x_token), Y, test_size=0.33, random_state=7\n",
    "    )\n",
    "\n",
    "    # fit model on training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7026be-b708-40d7-b9ed-80eab0131dda",
   "metadata": {},
   "source": [
    "### try bert/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7c66e-d7d1-4f4a-bfae-914b1a758219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import transformers\n",
    "\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\") # \"bert-large-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab5353-3bb2-4386-b534-0ed6a4cf5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_keras_model(x_train):\n",
    "#     input_word_ids = tf.keras.layers.Input(\n",
    "#         shape=(x_train.shape[1],), dtype=tf.int32, name=\"input_word_ids\"\n",
    "#     )\n",
    "    \n",
    "#     bert_layer = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\") # \"bert-large-uncased\"\n",
    "#     bert_outputs = bert_layer(input_word_ids)[0]\n",
    "#     pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")(bert_outputs[:, 0, :])\n",
    "# #     pred = tf.keras.layers.Dense(16, activation=\"softmax\")(bert_outputs[:, 0, :])\n",
    "\n",
    "#     model = tf.keras.models.Model(inputs=input_word_ids, outputs=pred)\n",
    "#     loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "#     metrics = tf.metrics.BinaryAccuracy()\n",
    "#     model.compile(\n",
    "#         loss=loss,\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "#         metrics=metrics,\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6ca23-b881-4ac7-92e0-8dc00919354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# maxlen = 512\n",
    "\n",
    "# for l in range(len(personality_type)):\n",
    "\n",
    "#     Y = list_personality[:, l]\n",
    "\n",
    "#     x_token = [\n",
    "#         tokenizer.encode(str(i), max_length=maxlen, pad_to_max_length=False, truncation=True)\n",
    "#         for i in X\n",
    "#     ]\n",
    "\n",
    "    \n",
    "#     # split data into train and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         np.array(x_token), Y, test_size=0.33, random_state=7\n",
    "#     )\n",
    "\n",
    "#     model = get_keras_model(X_train)\n",
    "#     model.summary()\n",
    "\n",
    "#     print(\"Started training\")\n",
    "#     model.fit(\n",
    "#         X_train,\n",
    "#         y_train,\n",
    "#         validation_data=(X_test, y_test),\n",
    "#         verbose=1,\n",
    "#         epochs=20,\n",
    "#         batch_size=batch_size,\n",
    "#         callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)],\n",
    "#     )\n",
    "\n",
    "#     # make predictions for test data\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     predictions = [round(value) for value in y_pred]\n",
    "#     # evaluate predictions\n",
    "#     accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "#     print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
