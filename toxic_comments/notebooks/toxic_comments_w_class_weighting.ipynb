{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport zipfile\n\nfrom imblearn.over_sampling import SMOTE\nfrom keras import models\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.python.framework.errors_impl import InvalidArgumentError\nfrom tensorflow.python.keras import backend as K","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T01:26:42.935218Z","iopub.execute_input":"2021-08-08T01:26:42.935661Z","iopub.status.idle":"2021-08-08T01:26:48.734182Z","shell.execute_reply.started":"2021-08-08T01:26:42.935545Z","shell.execute_reply":"2021-08-08T01:26:48.733311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_VOCAB = 500\n\ncntizer = CountVectorizer(analyzer=\"word\", max_features=MAX_VOCAB, max_df=0.7, min_df=0.1)\ntfizer = TfidfTransformer()\nsmote = SMOTE()\n\nstop_words = set(stopwords.words(\"english\"))\nstop_words.update(\n    [\n        \"zero\",\n        \"one\",\n        \"two\",\n        \"three\",\n        \"four\",\n        \"five\",\n        \"six\",\n        \"seven\",\n        \"eight\",\n        \"nine\",\n        \"ten\",\n        \"may\",\n        \"also\",\n        \"across\",\n        \"among\",\n        \"beside\",\n        \"however\",\n        \"yet\",\n        \"within\",\n    ]\n)\nre_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n\nstemmer = SnowballStemmer(\"english\")\n\n\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\" \", sentence)\n\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\n\ndef get_labels(df):\n    df = df.drop([\"id\", \"set\", \"toxicity\"], axis=1)\n    labels = list(df.columns)\n    labels.remove(\"comment_text\")\n    return labels\n\ndef pre_process(df):\n    labels = get_labels(df)\n\n    df[\"comment_text\"] = df[\"comment_text\"].apply(removeStopWords)\n    df[\"comment_text\"] = df[\"comment_text\"].apply(stemming)\n\n    sequences = df[\"comment_text\"].values\n    targets = df[labels].values\n\n    return sequences, targets, labels","metadata":{"execution":{"iopub.status.busy":"2021-08-08T01:26:54.783704Z","iopub.execute_input":"2021-08-08T01:26:54.784028Z","iopub.status.idle":"2021-08-08T01:26:54.803551Z","shell.execute_reply.started":"2021-08-08T01:26:54.783999Z","shell.execute_reply":"2021-08-08T01:26:54.802579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculating_class_weights(y_true):\n    number_dim = np.shape(y_true)[1]\n    weights = np.empty([number_dim, 2])\n    for i in range(number_dim):\n        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n    return weights\n\ndef get_weighted_loss(weights):\n    def weighted_loss(y_true, y_pred):\n        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n    return weighted_loss\n\ndef load_model(compile=True):\n    return models.load_model('toxic_model', compile=compile)\n\ndef classify(sample, model, labels):\n    predictions = {}\n    try:\n        prediction = model.predict([sample])\n    except InvalidArgumentError as e:\n        print(e.message)\n        return predictions\n\n    for l in range(len(labels)):\n        predictions[labels[l]] = True if prediction[0][l] > 0.5 else False\n    \n    return predictions\n\ndef get_encoder(sequences):\n    encoder = TextVectorization(\n        max_tokens=MAX_VOCAB, standardize=\"lower_and_strip_punctuation\"\n    )\n\n    encoder.adapt(sequences)\n    return encoder\n\n\ndef get_model(sequences, targets, loss_function):\n    encoder = get_encoder(sequences)\n\n    model = Sequential(\n        [\n            encoder,\n            Embedding(\n                input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True\n            ),\n            LSTM(64),\n            Dense(64, activation=\"relu\"),\n#             Dense(targets.shape[0], activation=\"sigmoid\"),\n            Dense(targets.shape[1], activation=\"sigmoid\"),\n        ]\n    )\n\n    model.compile(\n        loss=loss_function,\n        optimizer=Adam(1e-4),\n        metrics=[CategoricalCrossentropy()],\n    )\n\n    return model\n\n\ndef train(sequences, targets, labels, model):\n# def train(sequences, targets, labels, loss_function):\n#     model = get_model(sequences, targets, loss_function)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        sequences, targets, test_size=0.2, random_state=42\n    )\n\n    model.fit(\n        X_train,\n        y_train,\n        epochs=20,\n        batch_size=16,\n        validation_data=(X_test, y_test),\n        callbacks=[EarlyStopping(patience=5)],\n    )\n\n    pred = model.predict(X_test)\n\n    THRESH = 0.5\n    for i in range(len(labels)):\n        y_true = y_test[:, i]\n        y_pred = (pred[:, i] > THRESH).astype(int)\n        print(f\"======={labels[i]}\")\n        print(classification_report(y_true, y_pred))\n\n    model.save('toxic_model')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T01:26:56.326123Z","iopub.execute_input":"2021-08-08T01:26:56.326439Z","iopub.status.idle":"2021-08-08T01:26:56.341026Z","shell.execute_reply.started":"2021-08-08T01:26:56.32641Z","shell.execute_reply":"2021-08-08T01:26:56.339804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"READ_CSV = True\nif READ_CSV:\n    df = pd.read_csv('/kaggle/input/cleaned-toxic-comments/train_preprocessed.csv')\n    print(df.shape)\n# df = df.head(1000)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T01:26:57.72334Z","iopub.execute_input":"2021-08-08T01:26:57.72369Z","iopub.status.idle":"2021-08-08T01:26:59.20928Z","shell.execute_reply.started":"2021-08-08T01:26:57.723654Z","shell.execute_reply":"2021-08-08T01:26:59.208382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GENERATE_INPUTS = True\nif GENERATE_INPUTS:\n    sequences, targets, labels = pre_process(df)\nelse:\n    labels = get_labels(df)\n    sequences = np.load('sequences.p', allow_pickle=True)\n    targets = np.load('targets.p', allow_pickle=True)\nprint(sequences.shape)\nprint(targets.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T01:27:00.170048Z","iopub.execute_input":"2021-08-08T01:27:00.170367Z","iopub.status.idle":"2021-08-08T01:29:28.802879Z","shell.execute_reply.started":"2021-08-08T01:27:00.170337Z","shell.execute_reply":"2021-08-08T01:29:28.802015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAVE_INPUTS = True\nif SAVE_INPUTS:\n    with open('sequences.p', 'wb') as f:\n        np.save(f, sequences)\n    with open('targets.p', 'wb') as f:\n        np.save(f, targets)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T01:29:28.804371Z","iopub.execute_input":"2021-08-08T01:29:28.804722Z","iopub.status.idle":"2021-08-08T01:29:28.918972Z","shell.execute_reply.started":"2021-08-08T01:29:28.804685Z","shell.execute_reply":"2021-08-08T01:29:28.91809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USE_WEIGHTED_LOSS = False\n# if USE_WEIGHTED_LOSS:\n#     class_weights = calculating_class_weights(targets)\n#     print(class_weights)\n#     loss_function = get_weighted_loss(class_weights)\n# else:\n#     loss_function = BinaryCrossentropy(from_logits=False)\n\n# train(sequences, targets, labels, loss_function)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = calculating_class_weights(targets)\nweighted_model = get_model(sequences, targets, get_weighted_loss(class_weights))\nunweighted_model = get_model(sequences, targets, BinaryCrossentropy(from_logits=False))\n\n# classifier = VotingClassifier(estimators=[('weighted_model', weighted_model), ('unweighted_model', unweighted_model)], voting='hard')\ntrain(sequences, targets, labels, weighted_model)\ntrain(sequences, targets, labels, unweighted_model)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T01:29:28.920473Z","iopub.execute_input":"2021-08-08T01:29:28.920811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    sequences, targets, test_size=0.2, random_state=42\n)\n\nweighted_pred = weighted_model.predict(X_test)\nunweighted_model = unweighted_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.mean(weighted_pred, unweighted_model)\npred = np.mean( np.array([ weighted_pred, unweighted_model ]), axis=0 )\nTHRESH = 0.5\nfor i in range(len(labels)):\n    y_true = y_test[:, i]\n    y_pred = (pred[:, i] > THRESH).astype(int)\n    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred), display_labels=[0, 1])\n    print(labels[i])\n    print(classification_report(y_true, y_pred))\n    disp.plot()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ZIP_MODEL = True\n\ndef zipdir(path, ziph):\n    # ziph is zipfile handle\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ziph.write(os.path.join(root, file), \n                       os.path.relpath(os.path.join(root, file), \n                                       os.path.join(path, '..')))\n\nif ZIP_MODEL:\n    zipf = zipfile.ZipFile('toxic_model.zip', 'w', zipfile.ZIP_DEFLATED)\n    zipdir('toxic_model/', zipf)\n    zipf.close()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:36:41.810274Z","iopub.execute_input":"2021-08-06T19:36:41.810619Z","iopub.status.idle":"2021-08-06T19:36:41.907559Z","shell.execute_reply.started":"2021-08-06T19:36:41.810588Z","shell.execute_reply":"2021-08-06T19:36:41.906754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(compile=False)\nmodel.compile()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:01:53.380561Z","iopub.execute_input":"2021-08-06T20:01:53.380883Z","iopub.status.idle":"2021-08-06T20:01:57.863734Z","shell.execute_reply.started":"2021-08-06T20:01:53.380852Z","shell.execute_reply":"2021-08-06T20:01:57.862917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    sequences, targets, test_size=0.2, random_state=42\n)\n\n\npred = model.predict(X_test)\n\nTHRESH = 0.5\nfor i in range(len(labels)):\n    y_true = y_test[:, i]\n    y_pred = (pred[:, i] > THRESH).astype(int)\n    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred), display_labels=[0, 1])\n    print(labels[i])\n    print(classification_report(y_true, y_pred))\n    disp.plot()\n    plt.show()\n#     print(f\"======={labels[i]}\")\n#     print(confusion_matrix(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:14:50.635187Z","iopub.execute_input":"2021-08-06T20:14:50.63554Z","iopub.status.idle":"2021-08-06T20:14:59.001908Z","shell.execute_reply.started":"2021-08-06T20:14:50.635508Z","shell.execute_reply":"2021-08-06T20:14:59.000948Z"},"trusted":true},"execution_count":null,"outputs":[]}]}