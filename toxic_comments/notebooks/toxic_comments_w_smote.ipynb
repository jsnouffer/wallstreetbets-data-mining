{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom imblearn.over_sampling import ADASYN, BorderlineSMOTE, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom transformers import BertModel, BertTokenizer\nfrom typing import Callable, List, Optional, Tuple\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:37:15.494244Z","iopub.execute_input":"2021-08-07T16:37:15.495102Z","iopub.status.idle":"2021-08-07T16:37:24.450395Z","shell.execute_reply.started":"2021-08-07T16:37:15.494959Z","shell.execute_reply":"2021-08-07T16:37:24.449308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cleaned-toxic-comments/train_preprocessed.csv')\nprint(df.shape)\n\n# df = df.head(5000)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:37:28.545353Z","iopub.execute_input":"2021-08-07T16:37:28.545751Z","iopub.status.idle":"2021-08-07T16:37:30.196759Z","shell.execute_reply.started":"2021-08-07T16:37:28.54572Z","shell.execute_reply":"2021-08-07T16:37:30.19566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = df[\"comment_text\"].values\ntargets = df['identity_hate'].values\nprint(sequences.shape)\nprint(targets.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:37:31.481999Z","iopub.execute_input":"2021-08-07T16:37:31.482357Z","iopub.status.idle":"2021-08-07T16:37:31.49897Z","shell.execute_reply.started":"2021-08-07T16:37:31.482325Z","shell.execute_reply":"2021-08-07T16:37:31.497923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    sequences, targets, test_size=0.2, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:37:35.247231Z","iopub.execute_input":"2021-08-07T16:37:35.247597Z","iopub.status.idle":"2021-08-07T16:37:35.25499Z","shell.execute_reply.started":"2021-08-07T16:37:35.247566Z","shell.execute_reply":"2021-08-07T16:37:35.253338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        bert_tokenizer,\n        bert_model,\n        max_length: int = 60,\n        embedding_func: Optional[Callable[[torch.tensor], torch.tensor]] = None,\n    ):\n        self.tokenizer = bert_tokenizer\n        self.model = bert_model\n        self.model.eval()\n        self.max_length = max_length\n        self.embedding_func = embedding_func\n\n        if self.embedding_func is None:\n            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n\n    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:\n        # Tokenize the text with the provided tokenizer\n        tokenized_text = self.tokenizer.encode_plus(\n            text, add_special_tokens=True, max_length=self.max_length\n        )[\"input_ids\"]\n\n        # Create an attention mask telling BERT to use all words\n        attention_mask = [1] * len(tokenized_text)\n\n        # bert takes in a batch so we need to unsqueeze the rows\n        return (\n            torch.tensor(tokenized_text).unsqueeze(0),\n            torch.tensor(attention_mask).unsqueeze(0),\n        )\n\n    def _tokenize_and_predict(self, text: str) -> torch.tensor:\n        tokenized, attention_mask = self._tokenize(text)\n\n        embeddings = self.model(tokenized, attention_mask)\n        return self.embedding_func(embeddings)\n\n    def transform(self, text: List[str]):\n        if isinstance(text, pd.Series):\n            text = text.tolist()\n\n        with torch.no_grad():\n            return torch.stack([self._tokenize_and_predict(string) for string in text])\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n        return self","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:37:37.54331Z","iopub.execute_input":"2021-08-07T16:37:37.543857Z","iopub.status.idle":"2021-08-07T16:37:37.565101Z","shell.execute_reply.started":"2021-08-07T16:37:37.543811Z","shell.execute_reply":"2021-08-07T16:37:37.563651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_dataset = \"bert-base-uncased\"\n\n\ndef tfidf():\n    return Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer())])\n\n\ndef bert():\n    tokenizer = BertTokenizer.from_pretrained(bert_dataset)\n    bert_model = BertModel.from_pretrained(bert_dataset)\n    return BertTransformer(tokenizer, bert_model)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:37:39.770749Z","iopub.execute_input":"2021-08-07T16:37:39.77114Z","iopub.status.idle":"2021-08-07T16:37:39.780702Z","shell.execute_reply.started":"2021-08-07T16:37:39.771108Z","shell.execute_reply":"2021-08-07T16:37:39.779633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote = SMOTE(random_state=12)\nborderline_smote = BorderlineSMOTE(sampling_strategy=0.5,k_neighbors=5,random_state=42, kind=\"borderline-1\")\nrandom_undersampler = RandomUnderSampler(sampling_strategy=1, random_state=42)\nadasyn = ADASYN(sampling_strategy=0.5,random_state=42)\n\nmodel = Pipeline(\n    [\n        (\"vect\", CountVectorizer()),\n        (\"tfidf\", TfidfTransformer()),\n        (\"adasyn\", adasyn),\n        (\"smote\", borderline_smote),\n        (\"under-sampling\", random_undersampler),\n        (\"mnb\", MultinomialNB(alpha=0.1)),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:53:39.9382Z","iopub.execute_input":"2021-08-07T15:53:39.938772Z","iopub.status.idle":"2021-08-07T15:53:39.947814Z","shell.execute_reply.started":"2021-08-07T15:53:39.938721Z","shell.execute_reply":"2021-08-07T15:53:39.946424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = XGBClassifier()\nmodel = Pipeline(\n    [\n        (\n            \"union\",\n            FeatureUnion(\n                transformer_list=[(\"bert\", bert()), (\"tf_idf\", tfidf())]\n            ),\n        ),\n        (\"classifier\", classifier),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:37:43.584425Z","iopub.execute_input":"2021-08-07T16:37:43.584802Z","iopub.status.idle":"2021-08-07T16:38:07.243323Z","shell.execute_reply.started":"2021-08-07T16:37:43.584756Z","shell.execute_reply":"2021-08-07T16:38:07.242021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:38:11.407476Z","iopub.execute_input":"2021-08-07T16:38:11.407905Z","iopub.status.idle":"2021-08-07T16:51:09.989005Z","shell.execute_reply.started":"2021-08-07T16:38:11.407873Z","shell.execute_reply":"2021-08-07T16:51:09.987898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"THRESH = 0.5\npred = model.predict(X_test)\ny_pred = (pred > THRESH).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:51:44.607703Z","iopub.execute_input":"2021-08-07T16:51:44.608163Z","iopub.status.idle":"2021-08-07T16:55:45.767422Z","shell.execute_reply.started":"2021-08-07T16:51:44.608119Z","shell.execute_reply":"2021-08-07T16:55:45.766214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=[0, 1])\nprint(classification_report(y_test, y_pred))\ndisp.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:56:54.690236Z","iopub.execute_input":"2021-08-07T16:56:54.69062Z","iopub.status.idle":"2021-08-07T16:56:54.945133Z","shell.execute_reply.started":"2021-08-07T16:56:54.690591Z","shell.execute_reply":"2021-08-07T16:56:54.943699Z"},"trusted":true},"execution_count":null,"outputs":[]}]}